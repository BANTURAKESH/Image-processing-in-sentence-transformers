# -*- coding: utf-8 -*-
"""Deep learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TG4m-pTk4RDmvhHH3-L4iV9elsxrL2Iv
"""

# Neural Network







"""Neural Network for Handwriten Digits Classification

Deep Learning
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow
import tensorflow as tf
from tensorflow import  keras
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

(x_train,y_train),(x_test , y_test) = keras.datasets.mnist.load_data()



len(x_train)

len(y_train)

len(x_test)

x_train[0].shape

x_train[0]

plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(True)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    #plt.xlabel(mnist[x_test[i]])
plt.show()

plt.matshow(x_train[0])

y_train[7]

y_train[:5]

x_train.shape

y_train.shape

x_train = x_train/255
x_test = x_test/255



x_train_flattened = x_train.reshape(len(x_train),28*28)
x_test_flattened = x_test.reshape(len(x_test),28*28)

x_train[0]

x_train_flattened.shape

x_test_flattened.shape

x_train_flattened.shape

x_train_flattened[0]

"""#Build the model
* Creating a neural Network
* Building the neural network requires configuring the layers of the model, then compiling the model.
* The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.

* Most of deep learning consists of chaining together simple layers. Most layers, such as *tf.keras.layers.Dense*, have parameters that are learned during training.



"""

model = keras.Sequential([
    keras.layers.Dense(10,input_shape=(784,),activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(x_train_flattened,y_train,epochs=5)

"""# Evaluate accuracy on test data"""

model.evaluate(x_test_flattened,y_test)

plt.matshow(x_test[0])

"""# Prediction model """

y_predicted = model.predict(x_test_flattened) 
y_predicted[0]

np.argmax(y_predicted[0])

y_predicted_labels = [np.argmax(i) for i in y_predicted]
y_predicted_labels[:5]

y_test[:5]

"""# confusion_matrix"""

cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)
cm

import seaborn as sns
plt.figure(figsize=(10,7))
sns.heatmap(cm,annot=True,fmt='d',color= 'Blue')
plt.xlabel('predicted')
plt.ylabel('Truth')

"""#Adding  Hidden layer """

model = keras.Sequential([
    keras.layers.Dense(100,input_shape=(784,),activation = 'relu'),
    keras.layers.Dense(10,activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(x_train_flattened,y_train,epochs=5)

model.evaluate(x_test_flattened,y_test)



y_predicted = model.predict(x_test_flattened) 
y_predicted_labels = [np.argmax(i) for i in y_predicted]

cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)
cm

plt.figure(figsize=(10,7))
sns.heatmap(cm,annot=True,fmt='d',color= 'Blue')
plt.xlabel('predicted')
plt.ylabel('Truth')

"""# USING FLATTEN FUNCTION"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(100,activation = 'relu'),
    keras.layers.Dense(10,activation = 'sigmoid')
])

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(x_train,y_train,epochs=5)



"""# #Variants of Activation Function

#Sigmoid Function

# Equation : A = 1/(1 + e-x)
 * Value Range : 0 to 1
 * Uses : Usually used in output layer of a binary classification, where result is either 0 or 1.
 *  as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise.
"""

import math

def sigmoid(x):
  return 1/(1+math.exp(-x))

sigmoid(100)

sigmoid(-56)

sigmoid(0.5)

"""#Tanh Function 
* ![image.png](data:image/png;base64,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)

* Value Range :- -1 to +1
"""

def tanh(x):
  return (math.exp(x)-math.exp(-x)) / (math.exp(x)+math.exp(-x))

tanh(-56)

tanh(50)

"""#RELU Function 
* Equation :- A(x) = max(0,x).
* Value Range :- [0, inf)
*  It gives an output x if x is positive and 0 otherwise.
* negetive values gives "0"
* positive values gives same input value
"""

def relu(x):
  return max(0,x)

relu(-7)

relu(0)

relu(10)



"""# Convolutional Neural Network

# padding
"""

import tensorflow
from tensorflow import keras
from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D
from keras import Sequential
from keras.datasets import mnist

(x_train,y_train),(x_teat,y_test) = mnist.load_data()

model = Sequential()


model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",activation='relu',input_shape=(28,28,1)))
model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",activation='relu'))
model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",activation='relu'))

model.add(Flatten())

model.add(Dense(128,activation = "relu"))
model.add(Dense(128,activation = "softmax"))

model.summary()

"""#strides"""

model = Sequential()


model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",strides = (2,2),activation='relu',input_shape=(28,28,1)))
model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",strides = (2,2),activation='relu'))
model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",strides = (2,2),activation='relu'))

model.add(Flatten())

model.add(Dense(128,activation = "relu"))
model.add(Dense(128,activation = "softmax"))

model.summary()



"""# Pooling"""

model = Sequential()


model.add(Conv2D(32,kernel_size=(3,3),padding = "valid",activation='relu',input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding="valid"))
model.add(Conv2D(32,kernel_size=(3,3),padding = "valid" ,activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding="valid")) 
model.add(Flatten())

model.add(Dense(128,activation = "relu"))
model.add(Dense(128,activation = "softmax"))

model.summary()





































"""# Natural Language Processing with Transformers

#  SentenceTransformers

* SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings

* You can use this framework to compute sentence / text embeddings for more than 100 languages.
*  These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.

* The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models.

# projects

* Sentence Embeddings and Similarity project
* Semantic Search project 
* K-Mean Clustering on Text Data project
* Fast Clustering project
* Similar Research Paper Recommendation System project
* Extractive text summarization project

# Project 1- Sentence Embeddings and Similarity
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer,util
model = SentenceTransformer('all-MiniLM-L6-v2')



"""* Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers."""



sentences = ['the cat sits outside','the new movie is awesome','the new movie is really great','the dog bark on strange']

embeddings = model.encode(sentences=sentences,convert_to_tensor=True)

for sent,embed in zip(sentences,embeddings):
  print('sentence',sent)
  print('len(Embeddings:', len(embed))
  #print('Embeddingd:',embed)

"""#  cos_sim
* Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j. :return: Matrix with res[i][j] = cos_sim(a[i], b[j])

# util
* sentence_transformers.util defines different helpful functions to work with text embeddings.
"""

cosine_scores = util.cos_sim(embeddings,embeddings)

cosine_scores

sentences

"""# paraphrase mining
* Given a list of sentences / texts, this function performs paraphrase mining. It compares all sentences against all other sentences and returns a list with the pairs that have the highest cosine similarity score.

# Parameters
model – SentenceTransformer model for embedding computation

sentences – A list of strings (texts or sentences)

show_progress_bar – Plotting of a progress bar

batch_size – Number of texts that are encoded simultaneously by the model

query_chunk_size – Search for most similar pairs for #query_chunk_size at the same time. Decrease, to lower memory footprint (increases run-time).

corpus_chunk_size – Compare a sentence simultaneously against #corpus_chunk_size other sentences. Decrease, to lower memory footprint (increases run-time).

max_pairs – Maximal number of text pairs returned.

top_k – For each sentence, we retrieve up to top_k other sentences

score_function – Function for computing scores. By default, cosine similarity.

# Returns
Returns a list of triplets with the format [score, id1, id2]
"""

paraphrases = util.paraphrase_mining(model,sentences)

for sim in paraphrases[0:10]:
  score,i,j = sim
  print(sentences[i], '<>',sentences[j], "-->", score)
  print()



"""# Semantic Search 
* Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines which only find documents based on lexical matches, semantic search can also find synonyms.

* The idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space.

* At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found. These entries should have a high semantic overlap with the query.

# symmetric semantic search
* For symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content.

# asymmetric semantic search
* asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query
"""

from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')

import requests

"""
# corpus
* corpus is a collection of authentic text or audio organized into datasets. Authentic here means text written or audio spoken by a native of the language or dialect. A corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets. In natural language processing, a corpus contains text and speech data that can be used to train AI and machine learning systems"""

from requests.models import Response
responce = requests.get('https://raw.githubusercontent.com/laxmimerit/machine-learning-dataset/master/text-dataset-for-machine-learning/sbert-corpus.txt')
corpus  = responce.text.split('\r\n')

responce = requests.get('https://raw.githubusercontent.com/laxmimerit/machine-learning-dataset/master/text-dataset-for-machine-learning/sbert-queries.txt')
queries = responce.text.split('\r\n')

print(corpus)

print(queries)

corpus_embeddings = model.encode(corpus,convert_to_tensor=True)
queries_embeddings = model.encode(queries,convert_to_tensor=True)

# lets normolize vector for fast calucation
corpus_embeddings = util.normalize_embeddings(corpus_embeddings)
queries_embeddings = util.normalize_embeddings(queries_embeddings)

len(corpus_embeddings[0])

hits = util.semantic_search(queries_embeddings, corpus_embeddings,score_function=util.dot_score, top_k=3)

hits

for query, hit in zip( queries , hits):
  for q_hit in hit:
    id = q_hit['corpus_id']
    score = q_hit['score']

    print( query, "<>", corpus[id], "-->", score)

  print()













"""# K-Mean Clustering on Text Data"""

from sklearn.cluster import KMeans

model = SentenceTransformer('all-MiniLM-L6-v2')

import requests
response = requests.get('https://raw.githubusercontent.com/laxmimerit/machine-learning-dataset/master/text-dataset-for-machine-learning/sbert-corpus.txt')
corpus = response.text.split('\r\n')

len(corpus), print(corpus)

corpus_embeddings = model.encode(corpus)

num_clusters = 5
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

cluster_assignment

clustered_sentences = [[] for i in range(num_clusters)]
clustered_sentences

for sentence_id, cluster_id in enumerate(cluster_assignment):
  clustered_sentences[cluster_id].append(corpus[sentence_id])

for i, cluster in enumerate(clustered_sentences):
  print("cluster",i+1)
  print(cluster)
  print()

"""# Fast Clustering
* Agglomerative Clustering for larger datasets is quite slow, so it is only applicable for maybe a few thousand sentences.

* In fast_clustering.py we present a clustering algorithm that is tuned for large datasets (50k sentences in less than 5 seconds). In a large list of sentences it searches for local communities: A local community is a set of highly similar sentences.

* You can configure the threshold of cosine-similarity for which we consider two sentences as similar. Also, you can specify the minimal size for a local community. This allows you to get either large coarse-grained clusters or small fine-grained clusters.
"""



from sentence_transformers import SentenceTransformer, util

import pandas as pd 
import time

model = SentenceTransformer('all-MiniLM-L6-v2')



df = pd.read_csv('http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv', sep='\t')

df.head()

df.shape

sentences = df['question1'].tolist()[:1000]
len(sentences)

corpus_embeddings = model.encode(sentences, batch_size = 64, show_progress_bar=True,convert_to_tensor=True)

clusters = util.community_detection(corpus_embeddings, min_community_size=5,threshold=0.5)

for i, cluster in enumerate(clusters):
  print("\ncluster {}, #{} Questions" . format(i+1, len(cluster)))
  for id in cluster[0:3]:
    print('\t', sentences[id])
  print("\t","...")



"""# Similar research paper recommendation project"""



from sentence_transformers import SentenceTransformer, util
import os 
import json
import requests

response = requests.get('https://sbert.net/datasets/emnlp2016-2018.json')
papers = json.loads(response.text)

len(papers)

papers[0]

model = SentenceTransformer ('allenai-specter')

paper_texts = [paper['title'] + '[SEP]' + paper['abstract'] for paper in papers]

array_length = len(papers)
array_length

corpus_embeddings = model.encode(paper_texts, convert_to_tensor=True, show_progress_bar=True)



def search(title, abstract):
  query_embeddings = model.encode(title + '[SEP]' + abstract, convert_to_tensor=True)

  search_hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=3)[0]


  print("\n\n\nMost similar papers\n")
  for hit in search_hits:
    related_paper = papers[hit['corpus_id']]
    print(related_paper['title'])
    print(related_paper['abstract'])

title = "nlp"
abstract = "nlp"
search(title, abstract)



"""# Extractive text summarization project.

* Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary. 

* Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing.
"""



from sentence_transformers import SentenceTransformer

# Commented out IPython magic to ensure Python compatibility.
# %pip install LexRank



from LexRank import degree_centrality_scores

import nltk
nltk.download('punkt')
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

document = """
The technology at the heart of the most innovative progress in health care artificial intelligence (AI) is in a subdomain called machine learning (ML), which describes the use of software algorithms to identify patterns in very large datasets. ML has driven much of the progress of health care AI over the past 5 years, demonstrating impressive results in clinical decision support, patient monitoring and coaching, surgical assistance, patient care, and systems management. Clinicians in the near future will find themselves working with information networks on a scale well beyond the capacity of human beings to grasp, thereby necessitating the use of intelligent machines to analyze and interpret the complex interactions between data, patients, and clinical decision makers. However, as this technology becomes more powerful, it also becomes less transparent, and algorithmic decisions are therefore progressively more opaque. This is problematic because computers will increasingly be asked for answers to clinical questions that have no single right answer and that are open-ended, subjective, and value laden. As ML continues to make important contributions in a variety of clinical domains, clinicians will need to have a deeper understanding of the design, implementation, and evaluation of ML to ensure that current health care is not overly influenced by the agenda of technology entrepreneurs and venture capitalists. The aim of this article is to provide a nontechnical introduction to the concept of ML in the context of health care, the challenges that arise, and the resulting implications for clinicians.
"""

#Split the document into sentences
sentences = nltk.sent_tokenize(document)
print("Num sentences:", len(sentences))





#Compute the sentence embeddings
embeddings = model.encode(sentences, convert_to_tensor=True)
embeddings

#Compute the pair-wise cosine similarities
cos_scores = util.cos_sim(embeddings, embeddings).cpu().numpy()
cos_scores

#Compute the centrality for each sentence
centrality_scores = degree_centrality_scores(cos_scores, threshold=None)
centrality_scores

#We argsort so that the first element is the sentence with the highest score
most_central_sentence_indices = np.argsort(-centrality_scores)
most_central_sentence_indices



#Print the 5 sentences with the highest scores
print("\n\nSummary:")
for idx in most_central_sentence_indices[:2]:
    print(sentences[idx].strip())

